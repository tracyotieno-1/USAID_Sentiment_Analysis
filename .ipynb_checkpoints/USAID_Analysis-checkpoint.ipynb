{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reddit API credentials\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"CTY-7JaWjWmegP8ZSJcmnw\",      # Your Client ID\n",
    "    client_secret=\"8q3ftj_Lpi3p7Z8-DlRM8p4bTOSsVg\",  # Your Client Secret\n",
    "    user_agent=\"USAID_Scraper\"  # Custom identifier\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "usaid_queries = [\n",
    "    \"USAID\"\n",
    "]\n",
    "\n",
    "regions = [\n",
    "    \"Africa\", \"Middle East\", \"Asia\", \"Europe\", \"Latin America\", \"South America\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Searching for: USAID Africa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_9904\\2401397066.py:14: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  created_at_human = datetime.utcfromtimestamp(created_at_utc).strftime('%Y-%m-%d %H:%M:%S')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Searching for: USAID Middle East\n",
      "\n",
      "ðŸ”¹ Searching for: USAID Asia\n",
      "\n",
      "ðŸ”¹ Searching for: USAID Europe\n",
      "\n",
      "ðŸ”¹ Searching for: USAID Latin America\n",
      "\n",
      "ðŸ”¹ Searching for: USAID South America\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "# Fetch posts and comments\n",
    "for query in usaid_queries:\n",
    "    for region in regions:\n",
    "        full_query = f\"{query} {region}\"\n",
    "        print(f\"\\nðŸ”¹ Searching for: {full_query}\")\n",
    "\n",
    "        for post in reddit.subreddit(\"all\").search(full_query, limit=500, sort=\"new\"):\n",
    "            # Get top 5 comments\n",
    "            post.comments.replace_more(limit=0)  # Load all comments\n",
    "            top_comments = [comment.body for comment in post.comments.list()[:5]]\n",
    "            created_at_utc = post.created_utc\n",
    "            created_at_human = datetime.utcfromtimestamp(created_at_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "            post.author.name if post.author else \"Unknown\" \n",
    "\n",
    "            post_data = {\n",
    "                \"Post ID\": post.id,\n",
    "                \"region\": region,\n",
    "                \"Text\": post.selftext,\n",
    "                \"title\": post.title,\n",
    "                \"Author\":post.author.name,\n",
    "                \"subreddit\": post.subreddit.display_name,\n",
    "                \"score\": post.score,\n",
    "                \"Upvote Ratio\": post.upvote_ratio,\n",
    "                \"Number of Comments\": post.num_comments,\n",
    "                \"Created At\": created_at_utc,\n",
    "                \"url\": post.url,\n",
    "                \"Created Timestamp\": created_at_human,\n",
    "                \"comments\": \" || \".join(top_comments)  \n",
    "            }\n",
    "            results.append(post_data)  \n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"usaid_reddit_posts_New.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download the required NLTK data\n",
    "nltk.download('stopwords')  # For the stopwords\n",
    "nltk.download('punkt')      # For word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post ID</th>\n",
       "      <th>region</th>\n",
       "      <th>Text</th>\n",
       "      <th>title</th>\n",
       "      <th>Author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>Upvote Ratio</th>\n",
       "      <th>Number of Comments</th>\n",
       "      <th>Created At</th>\n",
       "      <th>url</th>\n",
       "      <th>Created Timestamp</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1is7s98</td>\n",
       "      <td>Africa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Trump's USAID Freeze: A Bid to Curb Corruption...</td>\n",
       "      <td>zsmithworks</td>\n",
       "      <td>conspiracywhatever</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1.739866e+09</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMibEFVX...</td>\n",
       "      <td>2025-02-18 08:07:55</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1is58gs</td>\n",
       "      <td>Africa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>His parents are Nazis too</td>\n",
       "      <td>anaiz_p</td>\n",
       "      <td>HuntingNazis</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1.739856e+09</td>\n",
       "      <td>https://i.redd.it/wefcfvo1rrje1.png</td>\n",
       "      <td>2025-02-18 05:19:46</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1is44wh</td>\n",
       "      <td>Africa</td>\n",
       "      <td>&gt;Ã¯Â»Â¿Ã¯Â»Â¿Ã¯Â»Â¿I am the Deputy Director of an overs...</td>\n",
       "      <td>Donald Trump and Elon Musk want US Government ...</td>\n",
       "      <td>Computer_Name</td>\n",
       "      <td>centrist</td>\n",
       "      <td>33</td>\n",
       "      <td>0.62</td>\n",
       "      <td>37</td>\n",
       "      <td>1.739852e+09</td>\n",
       "      <td>https://www.reddit.com/r/centrist/comments/1is...</td>\n",
       "      <td>2025-02-18 04:16:41</td>\n",
       "      <td>Anyone remember all of the pearl clutching whe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1irx1xf</td>\n",
       "      <td>Africa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>His parents are Nazis too</td>\n",
       "      <td>Tommy_Crash</td>\n",
       "      <td>u_Tommy_Crash</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1.739832e+09</td>\n",
       "      <td>https://i.redd.it/wefcfvo1rrje1.png</td>\n",
       "      <td>2025-02-17 22:35:46</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1irvqlz</td>\n",
       "      <td>Africa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>His parents are Nazis too</td>\n",
       "      <td>Violuthier</td>\n",
       "      <td>MarchAgainstNazis</td>\n",
       "      <td>813</td>\n",
       "      <td>1.00</td>\n",
       "      <td>25</td>\n",
       "      <td>1.739828e+09</td>\n",
       "      <td>https://i.redd.it/wefcfvo1rrje1.png</td>\n",
       "      <td>2025-02-17 21:41:33</td>\n",
       "      <td>Welcome to /r/MarchAgainstNazis!  \\n\\n**Please...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Post ID  region                                               Text  \\\n",
       "0  1is7s98  Africa                                                NaN   \n",
       "1  1is58gs  Africa                                                NaN   \n",
       "2  1is44wh  Africa  >Ã¯Â»Â¿Ã¯Â»Â¿Ã¯Â»Â¿I am the Deputy Director of an overs...   \n",
       "3  1irx1xf  Africa                                                NaN   \n",
       "4  1irvqlz  Africa                                                NaN   \n",
       "\n",
       "                                               title         Author  \\\n",
       "0  Trump's USAID Freeze: A Bid to Curb Corruption...    zsmithworks   \n",
       "1                          His parents are Nazis too        anaiz_p   \n",
       "2  Donald Trump and Elon Musk want US Government ...  Computer_Name   \n",
       "3                          His parents are Nazis too    Tommy_Crash   \n",
       "4                          His parents are Nazis too     Violuthier   \n",
       "\n",
       "            subreddit  score  Upvote Ratio  Number of Comments    Created At  \\\n",
       "0  conspiracywhatever      1          1.00                   0  1.739866e+09   \n",
       "1        HuntingNazis      1          1.00                   0  1.739856e+09   \n",
       "2            centrist     33          0.62                  37  1.739852e+09   \n",
       "3       u_Tommy_Crash      1          1.00                   0  1.739832e+09   \n",
       "4   MarchAgainstNazis    813          1.00                  25  1.739828e+09   \n",
       "\n",
       "                                                 url    Created Timestamp  \\\n",
       "0  https://news.google.com/rss/articles/CBMibEFVX...  2025-02-18 08:07:55   \n",
       "1                https://i.redd.it/wefcfvo1rrje1.png  2025-02-18 05:19:46   \n",
       "2  https://www.reddit.com/r/centrist/comments/1is...  2025-02-18 04:16:41   \n",
       "3                https://i.redd.it/wefcfvo1rrje1.png  2025-02-17 22:35:46   \n",
       "4                https://i.redd.it/wefcfvo1rrje1.png  2025-02-17 21:41:33   \n",
       "\n",
       "                                            comments  \n",
       "0                                                NaN  \n",
       "1                                                NaN  \n",
       "2  Anyone remember all of the pearl clutching whe...  \n",
       "3                                                NaN  \n",
       "4  Welcome to /r/MarchAgainstNazis!  \\n\\n**Please...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Try specifying a different encoding\n",
    "df = pd.read_csv(\"usaid_reddit_posts_New.csv\", encoding='latin1')  # You can also try 'ISO-8859-1' or 'cp1252'\n",
    "\n",
    "# If it works, the dataframe will load without errors\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER/nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method Downloader.download of <nltk.downloader.Downloader object at 0x00000280CF2F49B0>>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "\n",
    "# Specify the directory for NLTK data\n",
    "nltk_data_dir = os.path.expanduser(\"~/nltk_data\")\n",
    "nltk.data.path.append(nltk_data_dir)\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords', download_dir=nltk_data_dir)\n",
    "nltk.download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaned and saved.\n"
     ]
    }
   ],
   "source": [
    "# Define a cleaning function\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):  # Only process strings\n",
    "        text = text.lower()  # Convert to lowercase\n",
    "        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)  # Remove URLs\n",
    "        text = re.sub(r'\\W', ' ', text)  # Remove special characters and punctuation\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "        tokens = word_tokenize(text)  # Tokenize the text\n",
    "        filtered_words = [word for word in tokens if word not in stopwords.words('english')]  # Remove stopwords\n",
    "        return ' '.join(filtered_words)\n",
    "    else:\n",
    "        return \"\"  # Return an empty string for non-string values\n",
    "\n",
    "# Apply cleaning to the 'Text' column using .loc\n",
    "df.loc[:, 'Cleaned_Text'] = df['Text'].apply(clean_text)\n",
    "\n",
    "# Save the cleaned dataset\n",
    "df.to_csv(\"cleaned_usaid_data.csv\", index=False)\n",
    "print(\"Data cleaned and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from nltk>=3.9->textblob) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from nltk>=3.9->textblob) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cleaned_usaid_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'Cleaned_Text' column contains only strings\n",
    "df['Cleaned_Text'] = df['Cleaned_Text'].astype(str)\n",
    "\n",
    "# Sentiment analysis function\n",
    "def get_sentiment(text):\n",
    "    if not text or text.isspace():  # Check for empty or whitespace-only strings\n",
    "        return \"Neutral\"\n",
    "    analysis = TextBlob(text)\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return \"Positive\"\n",
    "    elif analysis.sentiment.polarity < 0:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "# Apply sentiment analysis to the cleaned text\n",
    "df['Sentiment'] = df['Cleaned_Text'].apply(get_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis completed and data saved.\n"
     ]
    }
   ],
   "source": [
    "# Save the dataset with sentiment results\n",
    "df.to_csv(\"sentiment_usaid_data.csv\", index=False)\n",
    "\n",
    "print(\"Sentiment analysis completed and data saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Post ID', 'region', 'Text', 'title', 'Author', 'subreddit', 'score',\n",
      "       'Upvote Ratio', 'Number of Comments', 'Created At', 'url',\n",
      "       'Created Timestamp', 'comments', 'Cleaned_Text', 'Sentiment'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm your USAID information assistant. Ask me anything about USAID.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what happened in usaid\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Run the chatbot\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m chatbot()\n",
      "Cell \u001b[1;32mIn[19], line 9\u001b[0m, in \u001b[0;36mchatbot\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124musaid\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m user_input:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Query your dataset for relevant information\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     relevant_data \u001b[38;5;241m=\u001b[39m data[data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124musaid\u001b[39m\u001b[38;5;124m'\u001b[39m, case\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, na\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)]\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m relevant_data\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBot: Here\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms some information about USAID:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "def chatbot():\n",
    "    print(\"Hello! I'm your USAID information assistant. Ask me anything about USAID.\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \").lower()\n",
    "        \n",
    "        if 'usaid' in user_input:\n",
    "            # Query your dataset for relevant information\n",
    "            relevant_data = data[data['Text'].str.contains('usaid', case=False, na=False)]\n",
    "            \n",
    "            if not relevant_data.empty:\n",
    "                print(\"Bot: Here's some information about USAID:\")\n",
    "                for index, row in relevant_data.head().iterrows():\n",
    "                    print(f\"- {row['title']}: {row['Text']}\")\n",
    "            else:\n",
    "                print(\"Bot: Sorry, I couldn't find any information about USAID.\")\n",
    "        else:\n",
    "            print(\"Bot: Sorry, that's beyond my current scope. Letâ€™s talk about something else.\")\n",
    "        \n",
    "        # Exit condition\n",
    "        if user_input in ['exit', 'quit', 'bye']:\n",
    "            print(\"Bot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "# Run the chatbot\n",
    "chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"sentiment_usaid_data.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm your USAID information assistant. Ask me anything about USAID.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Sorry, that's beyond my current scope. Letâ€™s talk about something else.\n",
      "Bot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "def chatbot():\n",
    "    print(\"Hello! I'm your USAID information assistant. Ask me anything about USAID.\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \").lower()\n",
    "        \n",
    "        if 'usaid' in user_input:\n",
    "            # Query your dataset for relevant information\n",
    "            relevant_data = df[df['Text'].str.contains('usaid', case=False, na=False)]\n",
    "            \n",
    "            if not relevant_data.empty:\n",
    "                print(\"Bot: Here's some information about USAID:\")\n",
    "                for index, row in relevant_data.head().iterrows():\n",
    "                    # Extract and summarize key points about USAID\n",
    "                    text = row['Text']\n",
    "                    if len(text) > 200:  # Summarize long text\n",
    "                        text = text[:200] + \"...\"\n",
    "                    print(f\"- {row['title']}: {text}\")\n",
    "            else:\n",
    "                print(\"Bot: Sorry, I couldn't find any information about USAID.\")\n",
    "        else:\n",
    "            print(\"Bot: Sorry, that's beyond my current scope. Letâ€™s talk about something else.\")\n",
    "        \n",
    "        # Exit condition\n",
    "        if user_input in ['exit', 'quit', 'bye']:\n",
    "            print(\"Bot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "# Run the chatbot\n",
    "chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Replace with your Deepseek API key\n",
    "DEEPSEEK_API_KEY = \"sk-183372fafc184722ba529f2285c3539b\"\n",
    "DEEPSEEK_SUMMARIZE_URL = \"https://api.deepseek.com/v1/summarize\"  # Replace with the actual API endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm your USAID information assistant. Ask me anything about USAID.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Sorry, that's beyond my current scope. Letâ€™s talk about something else.\n",
      "Bot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset\n",
    "data = pd.read_csv('sentiment_usaid_data.csv')\n",
    "\n",
    "# Function to summarize text using Deepseek API\n",
    "def summarize_with_deepseek(text):\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {DEEPSEEK_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"text\": text,\n",
    "        \"max_length\": 100  # Adjust the summary length as needed\n",
    "    }\n",
    "    response = requests.post(DEEPSEEK_SUMMARIZE_URL, headers=headers, json=payload)\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get(\"summary\", \"No summary available.\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return text[:100] + \"...\"  # Fallback to truncation if API fails\n",
    "\n",
    "# Chatbot function\n",
    "def chatbot():\n",
    "    print(\"Hello! I'm your USAID information assistant. Ask me anything about USAID.\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \").lower()\n",
    "        \n",
    "        if 'usaid' in user_input:\n",
    "            # Query your dataset for relevant information\n",
    "            relevant_data = data[data['Text'].str.contains('usaid', case=False, na=False)]\n",
    "            \n",
    "            if not relevant_data.empty:\n",
    "                print(\"Bot: Here's some information about USAID:\")\n",
    "                for index, row in relevant_data.head().iterrows():\n",
    "                    # Summarize the text using Deepseek API\n",
    "                    summary = summarize_with_deepseek(row['Text'])\n",
    "                    print(f\"- {row['title']}: {summary}\")\n",
    "            else:\n",
    "                print(\"Bot: Sorry, I couldn't find any information about USAID.\")\n",
    "        else:\n",
    "            print(\"Bot: Sorry, that's beyond my current scope. Letâ€™s talk about something else.\")\n",
    "        \n",
    "        # Exit condition\n",
    "        if user_input in ['exit', 'quit', 'bye']:\n",
    "            print(\"Bot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "# Run the chatbot\n",
    "chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (1.63.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from openai) (2.10.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIStatusError",
     "evalue": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAPIStatusError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[0;32m      5\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msk-183372fafc184722ba529f2285c3539b\u001b[39m\u001b[38;5;124m\"\u001b[39m, base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.deepseek.com\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m      8\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeepseek-chat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     10\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m     11\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m     12\u001b[0m     ],\n\u001b[0;32m     13\u001b[0m     stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_utils\\_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:879\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    839\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    876\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    877\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    878\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    880\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    881\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    882\u001b[0m             {\n\u001b[0;32m    883\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    884\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    885\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[0;32m    886\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    887\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    888\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    889\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    890\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    891\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m    892\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    893\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    894\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[0;32m    895\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    896\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m    897\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[0;32m    898\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    899\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[0;32m    900\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    901\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    902\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m    903\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    904\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m    905\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    906\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m    907\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    908\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    909\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    910\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    911\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    912\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    913\u001b[0m             },\n\u001b[0;32m    914\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    915\u001b[0m         ),\n\u001b[0;32m    916\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    917\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    918\u001b[0m         ),\n\u001b[0;32m    919\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    920\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    921\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    922\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1290\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1278\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1285\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1286\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1287\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1288\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1289\u001b[0m     )\n\u001b[1;32m-> 1290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:967\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    965\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    968\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    969\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    970\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    971\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    972\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m    973\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1071\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1068\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1070\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1071\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1074\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1075\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1079\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1080\u001b[0m )\n",
      "\u001b[1;31mAPIStatusError\u001b[0m: Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
     ]
    }
   ],
   "source": [
    "# Please install OpenAI SDK first: `pip3 install openai`\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"sk-183372fafc184722ba529f2285c3539b\", base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello\"},\n",
    "    ],\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post ID</th>\n",
       "      <th>region</th>\n",
       "      <th>Text</th>\n",
       "      <th>title</th>\n",
       "      <th>Author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>Upvote Ratio</th>\n",
       "      <th>Number of Comments</th>\n",
       "      <th>Created At</th>\n",
       "      <th>url</th>\n",
       "      <th>Created Timestamp</th>\n",
       "      <th>comments</th>\n",
       "      <th>Cleaned_Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1is7s98</td>\n",
       "      <td>Africa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Trump's USAID Freeze: A Bid to Curb Corruption...</td>\n",
       "      <td>zsmithworks</td>\n",
       "      <td>conspiracywhatever</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1.739866e+09</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMibEFVX...</td>\n",
       "      <td>2025-02-18 08:07:55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1is58gs</td>\n",
       "      <td>Africa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>His parents are Nazis too</td>\n",
       "      <td>anaiz_p</td>\n",
       "      <td>HuntingNazis</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1.739856e+09</td>\n",
       "      <td>https://i.redd.it/wefcfvo1rrje1.png</td>\n",
       "      <td>2025-02-18 05:19:46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1is44wh</td>\n",
       "      <td>Africa</td>\n",
       "      <td>&gt;Ã¯Â»Â¿Ã¯Â»Â¿Ã¯Â»Â¿I am the Deputy Director of an overs...</td>\n",
       "      <td>Donald Trump and Elon Musk want US Government ...</td>\n",
       "      <td>Computer_Name</td>\n",
       "      <td>centrist</td>\n",
       "      <td>33</td>\n",
       "      <td>0.62</td>\n",
       "      <td>37</td>\n",
       "      <td>1.739852e+09</td>\n",
       "      <td>https://www.reddit.com/r/centrist/comments/1is...</td>\n",
       "      <td>2025-02-18 04:16:41</td>\n",
       "      <td>Anyone remember all of the pearl clutching whe...</td>\n",
       "      <td>Ã¯ Ã¯ Ã¯ deputy director overseas technical offic...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1irx1xf</td>\n",
       "      <td>Africa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>His parents are Nazis too</td>\n",
       "      <td>Tommy_Crash</td>\n",
       "      <td>u_Tommy_Crash</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1.739832e+09</td>\n",
       "      <td>https://i.redd.it/wefcfvo1rrje1.png</td>\n",
       "      <td>2025-02-17 22:35:46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1irvqlz</td>\n",
       "      <td>Africa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>His parents are Nazis too</td>\n",
       "      <td>Violuthier</td>\n",
       "      <td>MarchAgainstNazis</td>\n",
       "      <td>813</td>\n",
       "      <td>1.00</td>\n",
       "      <td>25</td>\n",
       "      <td>1.739828e+09</td>\n",
       "      <td>https://i.redd.it/wefcfvo1rrje1.png</td>\n",
       "      <td>2025-02-17 21:41:33</td>\n",
       "      <td>Welcome to /r/MarchAgainstNazis!  \\n\\n**Please...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>1ircugs</td>\n",
       "      <td>South America</td>\n",
       "      <td>**Alkaline battery market** is anticipated to ...</td>\n",
       "      <td>Alkaline Battery Market is Dazzling Worldwide ...</td>\n",
       "      <td>TerribleSell2997</td>\n",
       "      <td>Nim2908</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1.739771e+09</td>\n",
       "      <td>https://www.reddit.com/r/Nim2908/comments/1irc...</td>\n",
       "      <td>2025-02-17 05:44:37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>alkaline battery market anticipated grow consi...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>1ircsqo</td>\n",
       "      <td>South America</td>\n",
       "      <td>**Building-integrated photovoltaics market** i...</td>\n",
       "      <td>Building-Integrated Photo voltaics (BIPV) Mark...</td>\n",
       "      <td>TerribleSell2997</td>\n",
       "      <td>Nim2908</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1.739771e+09</td>\n",
       "      <td>https://www.reddit.com/r/Nim2908/comments/1irc...</td>\n",
       "      <td>2025-02-17 05:41:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>building integrated photovoltaics market antic...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>1ircr2b</td>\n",
       "      <td>South America</td>\n",
       "      <td>**Energy storage system market** is anticipate...</td>\n",
       "      <td>Energy Storage System Market is Dazzling World...</td>\n",
       "      <td>TerribleSell2997</td>\n",
       "      <td>Nim2908</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1.739771e+09</td>\n",
       "      <td>https://www.reddit.com/r/Nim2908/comments/1irc...</td>\n",
       "      <td>2025-02-17 05:38:44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>energy storage system market anticipated grow ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>1ircqs2</td>\n",
       "      <td>South America</td>\n",
       "      <td># Perspective\\n\\nOne of the first things to de...</td>\n",
       "      <td>General Advice: Worldbuilding for D&amp;D, Part 1</td>\n",
       "      <td>AEDyssonance</td>\n",
       "      <td>Wyrlde</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.739771e+09</td>\n",
       "      <td>https://www.reddit.com/r/Wyrlde/comments/1ircq...</td>\n",
       "      <td>2025-02-17 05:38:13</td>\n",
       "      <td>I was yupping theough most of this, saying, Ã¢Â€...</td>\n",
       "      <td>perspective one first things decide going make...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>1ircq0e</td>\n",
       "      <td>South America</td>\n",
       "      <td>**Hydrogen energy storage market** is anticipa...</td>\n",
       "      <td>Hydrogen Energy Storage Market is Dazzling Wor...</td>\n",
       "      <td>TerribleSell2997</td>\n",
       "      <td>Nim2908</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1.739771e+09</td>\n",
       "      <td>https://www.reddit.com/r/Nim2908/comments/1irc...</td>\n",
       "      <td>2025-02-17 05:36:53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hydrogen energy storage market anticipated gro...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1435 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Post ID         region  \\\n",
       "0     1is7s98         Africa   \n",
       "1     1is58gs         Africa   \n",
       "2     1is44wh         Africa   \n",
       "3     1irx1xf         Africa   \n",
       "4     1irvqlz         Africa   \n",
       "...       ...            ...   \n",
       "1430  1ircugs  South America   \n",
       "1431  1ircsqo  South America   \n",
       "1432  1ircr2b  South America   \n",
       "1433  1ircqs2  South America   \n",
       "1434  1ircq0e  South America   \n",
       "\n",
       "                                                   Text  \\\n",
       "0                                                   NaN   \n",
       "1                                                   NaN   \n",
       "2     >Ã¯Â»Â¿Ã¯Â»Â¿Ã¯Â»Â¿I am the Deputy Director of an overs...   \n",
       "3                                                   NaN   \n",
       "4                                                   NaN   \n",
       "...                                                 ...   \n",
       "1430  **Alkaline battery market** is anticipated to ...   \n",
       "1431  **Building-integrated photovoltaics market** i...   \n",
       "1432  **Energy storage system market** is anticipate...   \n",
       "1433  # Perspective\\n\\nOne of the first things to de...   \n",
       "1434  **Hydrogen energy storage market** is anticipa...   \n",
       "\n",
       "                                                  title            Author  \\\n",
       "0     Trump's USAID Freeze: A Bid to Curb Corruption...       zsmithworks   \n",
       "1                             His parents are Nazis too           anaiz_p   \n",
       "2     Donald Trump and Elon Musk want US Government ...     Computer_Name   \n",
       "3                             His parents are Nazis too       Tommy_Crash   \n",
       "4                             His parents are Nazis too        Violuthier   \n",
       "...                                                 ...               ...   \n",
       "1430  Alkaline Battery Market is Dazzling Worldwide ...  TerribleSell2997   \n",
       "1431  Building-Integrated Photo voltaics (BIPV) Mark...  TerribleSell2997   \n",
       "1432  Energy Storage System Market is Dazzling World...  TerribleSell2997   \n",
       "1433      General Advice: Worldbuilding for D&D, Part 1      AEDyssonance   \n",
       "1434  Hydrogen Energy Storage Market is Dazzling Wor...  TerribleSell2997   \n",
       "\n",
       "               subreddit  score  Upvote Ratio  Number of Comments  \\\n",
       "0     conspiracywhatever      1          1.00                   0   \n",
       "1           HuntingNazis      1          1.00                   0   \n",
       "2               centrist     33          0.62                  37   \n",
       "3          u_Tommy_Crash      1          1.00                   0   \n",
       "4      MarchAgainstNazis    813          1.00                  25   \n",
       "...                  ...    ...           ...                 ...   \n",
       "1430             Nim2908      1          1.00                   0   \n",
       "1431             Nim2908      1          1.00                   0   \n",
       "1432             Nim2908      1          1.00                   0   \n",
       "1433              Wyrlde      2          1.00                   1   \n",
       "1434             Nim2908      1          1.00                   0   \n",
       "\n",
       "        Created At                                                url  \\\n",
       "0     1.739866e+09  https://news.google.com/rss/articles/CBMibEFVX...   \n",
       "1     1.739856e+09                https://i.redd.it/wefcfvo1rrje1.png   \n",
       "2     1.739852e+09  https://www.reddit.com/r/centrist/comments/1is...   \n",
       "3     1.739832e+09                https://i.redd.it/wefcfvo1rrje1.png   \n",
       "4     1.739828e+09                https://i.redd.it/wefcfvo1rrje1.png   \n",
       "...            ...                                                ...   \n",
       "1430  1.739771e+09  https://www.reddit.com/r/Nim2908/comments/1irc...   \n",
       "1431  1.739771e+09  https://www.reddit.com/r/Nim2908/comments/1irc...   \n",
       "1432  1.739771e+09  https://www.reddit.com/r/Nim2908/comments/1irc...   \n",
       "1433  1.739771e+09  https://www.reddit.com/r/Wyrlde/comments/1ircq...   \n",
       "1434  1.739771e+09  https://www.reddit.com/r/Nim2908/comments/1irc...   \n",
       "\n",
       "        Created Timestamp                                           comments  \\\n",
       "0     2025-02-18 08:07:55                                                NaN   \n",
       "1     2025-02-18 05:19:46                                                NaN   \n",
       "2     2025-02-18 04:16:41  Anyone remember all of the pearl clutching whe...   \n",
       "3     2025-02-17 22:35:46                                                NaN   \n",
       "4     2025-02-17 21:41:33  Welcome to /r/MarchAgainstNazis!  \\n\\n**Please...   \n",
       "...                   ...                                                ...   \n",
       "1430  2025-02-17 05:44:37                                                NaN   \n",
       "1431  2025-02-17 05:41:41                                                NaN   \n",
       "1432  2025-02-17 05:38:44                                                NaN   \n",
       "1433  2025-02-17 05:38:13  I was yupping theough most of this, saying, Ã¢Â€...   \n",
       "1434  2025-02-17 05:36:53                                                NaN   \n",
       "\n",
       "                                           Cleaned_Text Sentiment  \n",
       "0                                                   NaN   Neutral  \n",
       "1                                                   NaN   Neutral  \n",
       "2     Ã¯ Ã¯ Ã¯ deputy director overseas technical offic...  Positive  \n",
       "3                                                   NaN   Neutral  \n",
       "4                                                   NaN   Neutral  \n",
       "...                                                 ...       ...  \n",
       "1430  alkaline battery market anticipated grow consi...  Positive  \n",
       "1431  building integrated photovoltaics market antic...  Positive  \n",
       "1432  energy storage system market anticipated grow ...  Positive  \n",
       "1433  perspective one first things decide going make...  Positive  \n",
       "1434  hydrogen energy storage market anticipated gro...  Positive  \n",
       "\n",
       "[1435 rows x 15 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"sentiment_usaid_data.csv\")\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Post ID  region                                               Text  \\\n",
      "0  1is7s98  Africa  Trump's USAID Freeze: A Bid to Curb Corruption...   \n",
      "1  1is58gs  Africa                          His parents are Nazis too   \n",
      "2  1is44wh  Africa  >Ã¯Â»Â¿Ã¯Â»Â¿Ã¯Â»Â¿I am the Deputy Director of an overs...   \n",
      "3  1irx1xf  Africa                          His parents are Nazis too   \n",
      "4  1irvqlz  Africa                          His parents are Nazis too   \n",
      "\n",
      "                                               title         Author  \\\n",
      "0  Trump's USAID Freeze: A Bid to Curb Corruption...    zsmithworks   \n",
      "1                          His parents are Nazis too        anaiz_p   \n",
      "2  Donald Trump and Elon Musk want US Government ...  Computer_Name   \n",
      "3                          His parents are Nazis too    Tommy_Crash   \n",
      "4                          His parents are Nazis too     Violuthier   \n",
      "\n",
      "            subreddit  score  Upvote Ratio  Number of Comments    Created At  \\\n",
      "0  conspiracywhatever      1          1.00                   0  1.739866e+09   \n",
      "1        HuntingNazis      1          1.00                   0  1.739856e+09   \n",
      "2            centrist     33          0.62                  37  1.739852e+09   \n",
      "3       u_Tommy_Crash      1          1.00                   0  1.739832e+09   \n",
      "4   MarchAgainstNazis    813          1.00                  25  1.739828e+09   \n",
      "\n",
      "                                                 url    Created Timestamp  \\\n",
      "0  https://news.google.com/rss/articles/CBMibEFVX...  2025-02-18 08:07:55   \n",
      "1                https://i.redd.it/wefcfvo1rrje1.png  2025-02-18 05:19:46   \n",
      "2  https://www.reddit.com/r/centrist/comments/1is...  2025-02-18 04:16:41   \n",
      "3                https://i.redd.it/wefcfvo1rrje1.png  2025-02-17 22:35:46   \n",
      "4                https://i.redd.it/wefcfvo1rrje1.png  2025-02-17 21:41:33   \n",
      "\n",
      "                                            comments  \\\n",
      "0                                                NaN   \n",
      "1                                                NaN   \n",
      "2  Anyone remember all of the pearl clutching whe...   \n",
      "3                                                NaN   \n",
      "4  Welcome to /r/MarchAgainstNazis!  \\n\\n**Please...   \n",
      "\n",
      "                                        Cleaned_Text Sentiment  \n",
      "0  trumps usaid freeze a bid to curb corruption o...   Neutral  \n",
      "1                          his parents are nazis too   Neutral  \n",
      "2  i am the deputy director of an overseas techni...  Positive  \n",
      "3                          his parents are nazis too   Neutral  \n",
      "4                          his parents are nazis too   Neutral  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"sentiment_usaid_data.csv\")\n",
    "\n",
    "# Fill missing \"Text\" values with \"title\"\n",
    "df['Text'] = df['Text'].fillna(df['title'])\n",
    "\n",
    "# Remove any unwanted characters and convert to lowercase\n",
    "df['Cleaned_Text'] = df['Text'].str.replace(r'[^a-zA-Z0-9\\s]', '', regex=True).str.lower()\n",
    "\n",
    "# Drop rows where \"Cleaned_Text\" is still empty\n",
    "df = df.dropna(subset=['Cleaned_Text'])\n",
    "\n",
    "# Save cleaned data\n",
    "df.to_csv(\"cleaned_sentiment_usaid_data.csv\", index=False)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where Cleaned_Text is empty\n",
    "df = df.dropna(subset=['Cleaned_Text'])\n",
    "\n",
    "# Ensure text is treated as a string\n",
    "df['Cleaned_Text'] = df['Cleaned_Text'].astype(str)\n",
    "\n",
    "# Define features (text) and labels (sentiment)\n",
    "X = df['Cleaned_Text']\n",
    "y = df['Sentiment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where Cleaned_Text or Sentiment is missing\n",
    "df = df.dropna(subset=['Cleaned_Text', 'Sentiment'])\n",
    "\n",
    "# Convert all text data to string (avoids dtype issues)\n",
    "df['Cleaned_Text'] = df['Cleaned_Text'].astype(str)\n",
    "\n",
    "# Remove completely empty strings (if any)\n",
    "df = df[df['Cleaned_Text'].str.strip() != \"\"]\n",
    "\n",
    "# Define features (text) and labels (sentiment)\n",
    "X = df['Cleaned_Text']\n",
    "y = df['Sentiment']\n",
    "\n",
    "# Apply TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Limit vocab size\n",
    "X_tfidf = vectorizer.fit_transform(X)  # This should now work!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values in Cleaned_Text: 1\n",
      "Number of empty strings in Cleaned_Text: 1\n",
      "0    trumps usaid freeze a bid to curb corruption o...\n",
      "1                            his parents are nazis too\n",
      "2    i am the deputy director of an overseas techni...\n",
      "3                            his parents are nazis too\n",
      "4                            his parents are nazis too\n",
      "5    the dismantling of the us agency for internati...\n",
      "6    part of the governments argument for illegally...\n",
      "7    while elon is funded by the apartheid usaid fu...\n",
      "8                               no wonder he hit usaid\n",
      "9     fear and loathing of the energy cartel the fr...\n",
      "Name: Cleaned_Text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check for NaN values in Cleaned_Text\n",
    "print(\"Number of NaN values in Cleaned_Text:\", df['Cleaned_Text'].isna().sum())\n",
    "\n",
    "# Check for empty strings\n",
    "print(\"Number of empty strings in Cleaned_Text:\", (df['Cleaned_Text'].str.strip() == \"\").sum())\n",
    "\n",
    "# Print the first few Cleaned_Text values to inspect issues\n",
    "print(df['Cleaned_Text'].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final NaN check: 0\n",
      "Final empty string check: 0\n",
      "TF-IDF transformation successful!\n"
     ]
    }
   ],
   "source": [
    "# Drop NaN values in Cleaned_Text\n",
    "df = df.dropna(subset=['Cleaned_Text'])\n",
    "\n",
    "# Convert all text to string type\n",
    "df['Cleaned_Text'] = df['Cleaned_Text'].astype(str)\n",
    "\n",
    "# Remove completely empty strings\n",
    "df = df[df['Cleaned_Text'].str.strip() != \"\"]\n",
    "\n",
    "# Verify again after cleaning\n",
    "print(\"Final NaN check:\", df['Cleaned_Text'].isna().sum())\n",
    "print(\"Final empty string check:\", (df['Cleaned_Text'].str.strip() == \"\").sum())\n",
    "\n",
    "# Now apply TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = vectorizer.fit_transform(df['Cleaned_Text'])\n",
    "\n",
    "print(\"TF-IDF transformation successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Cleaned_Text'].astype(str)  # Convert all to strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final NaN check: 1\n",
      "Final empty string check: 1\n",
      "0       <class 'str'>\n",
      "1       <class 'str'>\n",
      "2       <class 'str'>\n",
      "3       <class 'str'>\n",
      "4       <class 'str'>\n",
      "            ...      \n",
      "1430    <class 'str'>\n",
      "1431    <class 'str'>\n",
      "1432    <class 'str'>\n",
      "1433    <class 'str'>\n",
      "1434    <class 'str'>\n",
      "Name: Cleaned_Text, Length: 1435, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Final NaN check:\", df['Cleaned_Text'].isna().sum())\n",
    "print(\"Final empty string check:\", (df['Cleaned_Text'].str.strip() == '').sum())\n",
    "print(df['Cleaned_Text'].apply(lambda x: type(x)))  # Check data types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Sentiment values: ['Neutral' 'Positive' 'Negative']\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique Sentiment values:\", df['Sentiment'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaN values in Cleaned_Text\n",
    "df = df.dropna(subset=['Cleaned_Text'])\n",
    "\n",
    "# Replace empty strings (after stripping spaces) with NaN and drop them\n",
    "df = df[df['Cleaned_Text'].str.strip() != '']\n",
    "\n",
    "# Convert Cleaned_Text to string type (for safety)\n",
    "X = df['Cleaned_Text'].astype(str)\n",
    "y = df['Sentiment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final NaN check: 0\n",
      "Final empty string check: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Final NaN check:\", df['Cleaned_Text'].isna().sum())\n",
    "print(\"Final empty string check:\", (df['Cleaned_Text'].str.strip() == '').sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = vectorizer.fit_transform(X)  # Should now work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exists: False\n",
      "Vectorizer exists: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Model exists:\", os.path.exists(\"sentiment_model.pkl\"))\n",
    "print(\"Vectorizer exists:\", os.path.exists(\"tfidf_vectorizer.pkl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final NaN check: 0\n",
      "Final empty string check: 0\n",
      "Whitespace-only strings: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_15588\\2570385358.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Cleaned_Text'].replace('', np.nan, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Convert everything to string (to avoid non-string issues)\n",
    "df['Cleaned_Text'] = df['Cleaned_Text'].astype(str)\n",
    "\n",
    "# Strip leading/trailing spaces and replace empty strings with NaN\n",
    "df['Cleaned_Text'] = df['Cleaned_Text'].str.strip()\n",
    "df['Cleaned_Text'].replace('', np.nan, inplace=True)\n",
    "\n",
    "# Drop NaN values again, just in case\n",
    "df.dropna(subset=['Cleaned_Text'], inplace=True)\n",
    "\n",
    "# Verify that all entries are valid\n",
    "print(\"Final NaN check:\", df['Cleaned_Text'].isna().sum())  # Must be 0\n",
    "print(\"Final empty string check:\", (df['Cleaned_Text'] == \"\").sum())  # Must be 0\n",
    "\n",
    "# Ensure there are no whitespace-only strings\n",
    "print(\"Whitespace-only strings:\", (df['Cleaned_Text'].str.isspace()).sum())  # Must be 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final NaN check: 0\n",
      "Final empty string check: 0\n",
      "Whitespace-only strings: 0\n"
     ]
    }
   ],
   "source": [
    "df = df.copy()  # Ensures we are modifying the original dataframe\n",
    "\n",
    "# Replace empty strings with NaN and drop them\n",
    "df.loc[df['Cleaned_Text'] == '', 'Cleaned_Text'] = np.nan\n",
    "df.dropna(subset=['Cleaned_Text'], inplace=True)\n",
    "\n",
    "# Verify again\n",
    "print(\"Final NaN check:\", df['Cleaned_Text'].isna().sum())  # Must be 0\n",
    "print(\"Final empty string check:\", (df['Cleaned_Text'] == \"\").sum())  # Must be 0\n",
    "print(\"Whitespace-only strings:\", (df['Cleaned_Text'].str.isspace()).sum())  # Must be 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_15588\\3893361113.py:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Cleaned_Text'].replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final NaN check: 0\n",
      "Final empty string check: 0\n",
      "Whitespace-only strings: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"cleaned_sentiment_usaid_data.csv\")\n",
    "\n",
    "# Drop rows where 'Sentiment' is missing\n",
    "df = df.dropna(subset=['Sentiment'])\n",
    "\n",
    "# Ensure 'Cleaned_Text' is treated as a string and strip whitespace\n",
    "df['Cleaned_Text'] = df['Cleaned_Text'].astype(str).str.strip()\n",
    "\n",
    "# Replace empty strings or whitespace-only strings with NaN\n",
    "df['Cleaned_Text'].replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "# Drop rows with NaN values in 'Cleaned_Text'\n",
    "df.dropna(subset=['Cleaned_Text'], inplace=True)\n",
    "\n",
    "# **Final Verification**\n",
    "print(\"Final NaN check:\", df['Cleaned_Text'].isna().sum())  # Should be 0\n",
    "print(\"Final empty string check:\", (df['Cleaned_Text'] == \"\").sum())  # Should be 0\n",
    "print(\"Whitespace-only strings:\", (df['Cleaned_Text'].str.isspace()).sum())  # Should be 0\n",
    "\n",
    "# Save the cleaned dataset again\n",
    "df.to_csv(\"cleaned_sentiment_usaid_data_fixed.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Convert text into numerical representation using TF-IDF\u001b[39;00m\n\u001b[0;32m      5\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m X_tfidf \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCleaned_Text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTF-IDF transformation successful!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2091\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2084\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2085\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2086\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2087\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2088\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2089\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2090\u001b[0m )\n\u001b[1;32m-> 2091\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_transform(raw_documents)\n\u001b[0;32m   2092\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2093\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2094\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1364\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1369\u001b[0m             )\n\u001b[0;32m   1370\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1372\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_)\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1375\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1259\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1257\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1258\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1259\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1260\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1261\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:103\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text processing steps to go from\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03ma single document to ngrams, with or without tokenizing or preprocessing.\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A sequence of tokens, possibly with pairs, triples, etc.\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 103\u001b[0m     doc \u001b[38;5;241m=\u001b[39m decoder(doc)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m analyzer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    105\u001b[0m     doc \u001b[38;5;241m=\u001b[39m analyzer(doc)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:236\u001b[0m, in \u001b[0;36m_VectorizerMixin.decode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    233\u001b[0m     doc \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_error)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;129;01mis\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan:\n\u001b[1;32m--> 236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    238\u001b[0m     )\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "\u001b[1;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "# Load fixed dataset\n",
    "df = pd.read_csv(\"cleaned_sentiment_usaid_data_fixed.csv\")\n",
    "\n",
    "# Convert text into numerical representation using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = vectorizer.fit_transform(df['Cleaned_Text'])\n",
    "\n",
    "print(\"TF-IDF transformation successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problematic rows:\n",
      "      Post ID       region                                               Text  \\\n",
      "363  1irx3ri  Middle East  Ã˜Â¨Ã˜Â§Ã˜Â³Ã™Â…",
      "Ã™ÂŠ Ã™ÂˆÃ˜Â¨Ã˜Â§Ã˜Â³Ã™Â…",
      " Ã˜Â¬Ã™Â…",
      "Ã™ÂŠÃ˜Â¹ Ã˜Â§Ã™Â„Ã˜Â§Ã˜Â¯Ã˜Â§Ã˜Â±Ã™ÂŠÃ™...   \n",
      "434  1irojsd  Middle East                                                  ?   \n",
      "\n",
      "                                                 title             Author  \\\n",
      "363                                  Ã˜Â´Ã™ÂƒÃ˜Â±Ã˜Â§ Ã™Â„Ã™ÂŠÃ™ÂƒÃ™Â…",
      "  Vast-Display-8431   \n",
      "434  Any ex-muslims here from the Middle East? I do...      maybeoneday20   \n",
      "\n",
      "    subreddit  score  Upvote Ratio  Number of Comments    Created At  \\\n",
      "363  Egypt360     12          0.93                  11  1.739832e+09   \n",
      "434  exmuslim     11          1.00                  33  1.739811e+09   \n",
      "\n",
      "                                                   url    Created Timestamp  \\\n",
      "363               https://i.redd.it/b038lyaf1sje1.jpeg  2025-02-17 22:37:57   \n",
      "434  https://www.reddit.com/r/exmuslim/comments/1ir...  2025-02-17 16:58:00   \n",
      "\n",
      "                                              comments  \\\n",
      "363  Ã˜Â§Ã™Â„Ã˜Â¹Ã™ÂÃ™Âˆ Ã™ÂŠÃ˜Â§ Ã˜ÂµÃ˜Â§Ã˜Â­Ã˜Â¨Ã™ÂŠ Ã˜Â§Ã™ÂŠ Ã˜Â®Ã˜Â¯Ã™Â…",
      "Ã˜Â© || Ã˜Â¹...   \n",
      "434   If your post is a meme, image, TikTok etc... ...   \n",
      "\n",
      "                               Cleaned_Text Sentiment  \n",
      "363  Â…",
      " Â…",
      " Â…",
      "   Â…",
      "   Â…",
      "Â…",
      "     Â…",
      "     Â…",
      " Â    Â…",
      "  \\nÂ…",
      "    Neutral  \n",
      "434                                     NaN   Neutral  \n",
      "Dataset cleaned! Try running TF-IDF now.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"cleaned_sentiment_usaid_data.csv\")\n",
    "\n",
    "# Identify rows causing the issue\n",
    "problematic_rows = df[df['Cleaned_Text'].isna() | (df['Cleaned_Text'].str.strip() == \"\")]\n",
    "print(\"Problematic rows:\\n\", problematic_rows)\n",
    "\n",
    "# If there are any, remove them\n",
    "df = df.dropna(subset=['Cleaned_Text'])\n",
    "df = df[df['Cleaned_Text'].str.strip() != \"\"]\n",
    "\n",
    "# Convert everything to string (just in case some are non-string types)\n",
    "df['Cleaned_Text'] = df['Cleaned_Text'].astype(str)\n",
    "\n",
    "# Save cleaned file\n",
    "df.to_csv(\"final_cleaned_sentiment_usaid_data.csv\", index=False)\n",
    "\n",
    "print(\"Dataset cleaned! Try running TF-IDF now.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF transformation successful!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df = pd.read_csv(\"final_cleaned_sentiment_usaid_data.csv\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = vectorizer.fit_transform(df['Cleaned_Text'])\n",
    "\n",
    "print(\"TF-IDF transformation successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    trumps usaid freeze a bid to curb corruption o...\n",
      "1                            his parents are nazis too\n",
      "2    i am the deputy director of an overseas techni...\n",
      "3                            his parents are nazis too\n",
      "4                            his parents are nazis too\n",
      "5    the dismantling of the us agency for internati...\n",
      "6    part of the governments argument for illegally...\n",
      "7    while elon is funded by the apartheid usaid fu...\n",
      "8                               no wonder he hit usaid\n",
      "9     fear and loathing of the energy cartel the fr...\n",
      "Name: Cleaned_Text, dtype: object\n",
      "\n",
      "Data Type Check: object\n"
     ]
    }
   ],
   "source": [
    "print(df['Cleaned_Text'].head(10))  # Show first 10 rows\n",
    "print(\"\\nData Type Check:\", df['Cleaned_Text'].dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš¨ Bad Data Found:\n",
      "      Post ID       region Text  \\\n",
      "434  1irojsd  Middle East    ?   \n",
      "\n",
      "                                                 title         Author  \\\n",
      "434  Any ex-muslims here from the Middle East? I do...  maybeoneday20   \n",
      "\n",
      "    subreddit  score  Upvote Ratio  Number of Comments    Created At  \\\n",
      "434  exmuslim     11           1.0                  33  1.739811e+09   \n",
      "\n",
      "                                                   url    Created Timestamp  \\\n",
      "434  https://www.reddit.com/r/exmuslim/comments/1ir...  2025-02-17 16:58:00   \n",
      "\n",
      "                                              comments Cleaned_Text Sentiment  \n",
      "434   If your post is a meme, image, TikTok etc... ...          NaN   Neutral  \n"
     ]
    }
   ],
   "source": [
    "# Check if any values are not strings\n",
    "bad_rows = df[~df['Cleaned_Text'].apply(lambda x: isinstance(x, str))]\n",
    "print(\"\\nðŸš¨ Bad Data Found:\\n\", bad_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Data is now guaranteed to be clean!\n"
     ]
    }
   ],
   "source": [
    "df['Cleaned_Text'] = df['Cleaned_Text'].astype(str)\n",
    "df['Cleaned_Text'] = df['Cleaned_Text'].apply(lambda x: x.strip())\n",
    "\n",
    "print(\"\\nâœ… Data is now guaranteed to be clean!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ‰ TF-IDF Sample Transformation Successful!\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "sample_texts = df['Cleaned_Text'].head(5)  # Small sample\n",
    "X_tfidf_sample = vectorizer.fit_transform(sample_texts)\n",
    "\n",
    "print(\"ðŸŽ‰ TF-IDF Sample Transformation Successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to numerical representation using TF-IDF (FIT it first)\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = vectorizer.fit_transform(df['Cleaned_Text'])  # Use fit_transform instead of transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8466898954703833\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.00      0.00      0.00        33\n",
      "     Neutral       0.90      0.89      0.90        73\n",
      "    Positive       0.83      0.98      0.90       181\n",
      "\n",
      "    accuracy                           0.85       287\n",
      "   macro avg       0.58      0.62      0.60       287\n",
      "weighted avg       0.75      0.85      0.80       287\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and vectorizer saved successfully!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# Define features (text) and labels (sentiment)\n",
    "X = df['Cleaned_Text']\n",
    "y = df['Sentiment']\n",
    "\n",
    "# Convert text to numerical representation using TF-IDF (already done)\n",
    "X_tfidf = vectorizer.transform(X)\n",
    "\n",
    "# Split dataset into training (80%) and testing (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the trained model and vectorizer\n",
    "joblib.dump(model, \"sentiment_model.pkl\")\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")\n",
    "\n",
    "print(\"Model and vectorizer saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment\n",
      "Positive    934\n",
      "Neutral     357\n",
      "Negative    144\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"sentiment_usaid_data.csv\")  # Load dataset again\n",
    "print(df['Sentiment'].value_counts())  # Check class distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.00      0.00      0.00       111\n",
      "     Neutral       0.92      0.95      0.94       284\n",
      "    Positive       0.87      0.99      0.92       753\n",
      "\n",
      "    accuracy                           0.88      1148\n",
      "   macro avg       0.60      0.65      0.62      1148\n",
      "weighted avg       0.80      0.88      0.84      1148\n",
      "\n",
      "Test Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.00      0.00      0.00        33\n",
      "     Neutral       0.90      0.89      0.90        73\n",
      "    Positive       0.83      0.98      0.90       181\n",
      "\n",
      "    accuracy                           0.85       287\n",
      "   macro avg       0.58      0.62      0.60       287\n",
      "weighted avg       0.75      0.85      0.80       287\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "print(\"Train Set Performance:\")\n",
    "print(classification_report(y_train, y_pred_train))\n",
    "\n",
    "print(\"Test Set Performance:\")\n",
    "print(classification_report(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Neutral': 72, 'Positive': 215}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "unique, counts = np.unique(model.predict(X_test), return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X_tfidf, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 5000\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size:\", len(vectorizer.vocabulary_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (0.12.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.5.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Apply oversampling\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X_tfidf, y)\n",
    "\n",
    "# Train your model again with balanced data\n",
    "model.fit(X_resampled, y_resampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Negative': 36, 'Neutral': 76, 'Positive': 175}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(model.predict(X_test), return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.92      1.00      0.96        33\n",
      "     Neutral       0.96      1.00      0.98        73\n",
      "    Positive       1.00      0.97      0.98       181\n",
      "\n",
      "    accuracy                           0.98       287\n",
      "   macro avg       0.96      0.99      0.97       287\n",
      "weighted avg       0.98      0.98      0.98       287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X_tfidf, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_resampled, y_resampled = rus.fit_resample(X_tfidf, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw prediction: Neutral\n",
      "Comment: USAID is doing a great job supporting communities!\n",
      "Predicted Sentiment: Neutral\n",
      "\n",
      "Raw prediction: Neutral\n",
      "Comment: I think cutting USAID funding was a terrible decision.\n",
      "Predicted Sentiment: Neutral\n",
      "\n",
      "Raw prediction: Neutral\n",
      "Comment: This program is wasting taxpayer money!\n",
      "Predicted Sentiment: Neutral\n",
      "\n",
      "Raw prediction: Neutral\n",
      "Comment: USAID has been instrumental in helping disaster victims.\n",
      "Predicted Sentiment: Neutral\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(comment):\n",
    "    comment_tfidf = vectorizer.transform([comment])  # Convert to TF-IDF\n",
    "    prediction = model.predict(comment_tfidf)[0]  # Get prediction label\n",
    "    \n",
    "    # Print raw prediction for debugging\n",
    "    print(f\"Raw prediction: {prediction}\")\n",
    "    \n",
    "    # Adjusted mapping\n",
    "    sentiment_mapping = {\n",
    "        \"Negative\": \"Negative\",\n",
    "        \"Neutral\": \"Neutral\",\n",
    "        \"Positive\": \"Positive\"\n",
    "    }\n",
    "    \n",
    "    return sentiment_mapping.get(prediction, \"Unknown\")  # Map prediction\n",
    "\n",
    "# Test with new Reddit comments\n",
    "test_comments = [\n",
    "    \"USAID is doing a great job supporting communities!\",\n",
    "    \"I think cutting USAID funding was a terrible decision.\",\n",
    "    \"This program is wasting taxpayer money!\",\n",
    "    \"USAID has been instrumental in helping disaster victims.\"\n",
    "]\n",
    "\n",
    "# Predict sentiment for each comment\n",
    "for comment in test_comments:\n",
    "    sentiment = predict_sentiment(comment)\n",
    "    print(f\"Comment: {comment}\\nPredicted Sentiment: {sentiment}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
