{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reddit API credentials\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"CTY-7JaWjWmegP8ZSJcmnw\",      # Your Client ID\n",
    "    client_secret=\"8q3ftj_Lpi3p7Z8-DlRM8p4bTOSsVg\",  # Your Client Secret\n",
    "    user_agent=\"USAID_Scraper\"  # Custom identifier\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "usaid_queries = [\n",
    "    \"USAID\"\n",
    "]\n",
    "\n",
    "regions = [\n",
    "    \"Africa\", \"Middle East\", \"Asia\", \"Europe\", \"Latin America\", \"South America\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Searching for: USAID Africa\n",
      "\n",
      "🔹 Searching for: USAID Middle East\n",
      "\n",
      "🔹 Searching for: USAID Asia\n",
      "\n",
      "🔹 Searching for: USAID Europe\n",
      "\n",
      "🔹 Searching for: USAID Latin America\n",
      "\n",
      "🔹 Searching for: USAID South America\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "# Fetch posts and comments\n",
    "for query in usaid_queries:\n",
    "    for region in regions:\n",
    "        full_query = f\"{query} {region}\"\n",
    "        print(f\"\\n🔹 Searching for: {full_query}\")\n",
    "\n",
    "        for post in reddit.subreddit(\"all\").search(full_query, limit=500, sort=\"new\"):\n",
    "            # Get top 5 comments\n",
    "            post.comments.replace_more(limit=0)  # Load all comments\n",
    "            top_comments = [comment.body for comment in post.comments.list()[:5]]\n",
    "            created_at_utc = post.created_utc\n",
    "            created_at_human = datetime.utcfromtimestamp(created_at_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            post.author.name if post.author else \"Unknown\" \n",
    "\n",
    "            post_data = {\n",
    "                \"Post ID\": post.id,\n",
    "                \"region\": region,\n",
    "                \"Text\": post.selftext,\n",
    "                \"title\": post.title,\n",
    "                \"Author\":post.author.name,\n",
    "                \"subreddit\": post.subreddit.display_name,\n",
    "                \"score\": post.score,\n",
    "                \"Upvote Ratio\": post.upvote_ratio,\n",
    "                \"Number of Comments\": post.num_comments,\n",
    "                \"Created At\": created_at_utc,\n",
    "                \"url\": post.url,\n",
    "                \"Created Timestamp\": created_at_human,\n",
    "                \"comments\": \" || \".join(top_comments)  \n",
    "            }\n",
    "            results.append(post_data)  \n",
    "#df = pd.DataFrame(results)\n",
    "df.to_csv(\"usaid_reddit_posts_New.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\perpe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\perpe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download the required NLTK data\n",
    "nltk.download('stopwords')  # For the stopwords\n",
    "nltk.download('punkt')      # For word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post ID</th>\n",
       "      <th>region</th>\n",
       "      <th>Text</th>\n",
       "      <th>title</th>\n",
       "      <th>Author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>Upvote Ratio</th>\n",
       "      <th>Number of Comments</th>\n",
       "      <th>Created At</th>\n",
       "      <th>url</th>\n",
       "      <th>Created Timestamp</th>\n",
       "      <th>comments</th>\n",
       "      <th>Unnamed: 13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1iis138</td>\n",
       "      <td>Africa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Real Reason Musk is After USAID: They Help...</td>\n",
       "      <td>shake1010</td>\n",
       "      <td>MarchAgainstNazis</td>\n",
       "      <td>860</td>\n",
       "      <td>0.99</td>\n",
       "      <td>21</td>\n",
       "      <td>1738808995</td>\n",
       "      <td>https://i.redd.it/arjpjejvjfhe1.jpeg</td>\n",
       "      <td>2/6/2025 2:29</td>\n",
       "      <td>Welcome to /r/MarchAgainstNazis!  \\n\\n**Please...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1iiontm</td>\n",
       "      <td>Africa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Usaid Delivering Foreign Aid to starving child...</td>\n",
       "      <td>trailer8k</td>\n",
       "      <td>Trailerclub</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1738799255</td>\n",
       "      <td>https://v.redd.it/w5xmfqsvqehe1</td>\n",
       "      <td>2/5/2025 23:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1iinl7c</td>\n",
       "      <td>Africa</td>\n",
       "      <td>How USAid has been spending taxpayer money\\n\\n...</td>\n",
       "      <td>How has USAID been spending taxpayer money?</td>\n",
       "      <td>ProtectedHologram</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>38</td>\n",
       "      <td>0.75</td>\n",
       "      <td>33</td>\n",
       "      <td>1738796394</td>\n",
       "      <td>https://www.reddit.com/r/conspiracy/comments/1...</td>\n",
       "      <td>2/5/2025 22:59</td>\n",
       "      <td>###[Meta] Sticky Comment\\n\\n[Rule 2](https://w...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1iini2u</td>\n",
       "      <td>Africa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Here's my list of exposed corrupt money launde...</td>\n",
       "      <td>Believe_143</td>\n",
       "      <td>LibTears</td>\n",
       "      <td>183</td>\n",
       "      <td>0.99</td>\n",
       "      <td>23</td>\n",
       "      <td>1738796165</td>\n",
       "      <td>https://i.redd.it/qt4m216qhehe1.jpeg</td>\n",
       "      <td>2/5/2025 22:56</td>\n",
       "      <td>And yet...hurricane victims in NC and fire vic...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1iijyzc</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Source: https://x.com/kenklippenstein/status/1...</td>\n",
       "      <td>Some people say that USAIDâs aid is exclusiv...</td>\n",
       "      <td>SnooAdvice725</td>\n",
       "      <td>Lebanese</td>\n",
       "      <td>123</td>\n",
       "      <td>0.98</td>\n",
       "      <td>12</td>\n",
       "      <td>1738787376</td>\n",
       "      <td>https://i.redd.it/esuop84lrdhe1.jpeg</td>\n",
       "      <td>2/5/2025 20:29</td>\n",
       "      <td>So they're interfering in international politi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Post ID  region                                               Text  \\\n",
       "0  1iis138  Africa                                                NaN   \n",
       "1  1iiontm  Africa                                                NaN   \n",
       "2  1iinl7c  Africa  How USAid has been spending taxpayer money\\n\\n...   \n",
       "3  1iini2u  Africa                                                NaN   \n",
       "4  1iijyzc  Africa  Source: https://x.com/kenklippenstein/status/1...   \n",
       "\n",
       "                                               title             Author  \\\n",
       "0  The Real Reason Musk is After USAID: They Help...          shake1010   \n",
       "1  Usaid Delivering Foreign Aid to starving child...          trailer8k   \n",
       "2        How has USAID been spending taxpayer money?  ProtectedHologram   \n",
       "3  Here's my list of exposed corrupt money launde...        Believe_143   \n",
       "4  Some people say that USAIDâs aid is exclusiv...      SnooAdvice725   \n",
       "\n",
       "           subreddit score Upvote Ratio Number of Comments  Created At  \\\n",
       "0  MarchAgainstNazis   860         0.99                 21  1738808995   \n",
       "1        Trailerclub     1            1                  0  1738799255   \n",
       "2         conspiracy    38         0.75                 33  1738796394   \n",
       "3           LibTears   183         0.99                 23  1738796165   \n",
       "4           Lebanese   123         0.98                 12  1738787376   \n",
       "\n",
       "                                                 url Created Timestamp  \\\n",
       "0               https://i.redd.it/arjpjejvjfhe1.jpeg     2/6/2025 2:29   \n",
       "1                    https://v.redd.it/w5xmfqsvqehe1    2/5/2025 23:47   \n",
       "2  https://www.reddit.com/r/conspiracy/comments/1...    2/5/2025 22:59   \n",
       "3               https://i.redd.it/qt4m216qhehe1.jpeg    2/5/2025 22:56   \n",
       "4               https://i.redd.it/esuop84lrdhe1.jpeg    2/5/2025 20:29   \n",
       "\n",
       "                                            comments Unnamed: 13  \n",
       "0  Welcome to /r/MarchAgainstNazis!  \\n\\n**Please...         NaN  \n",
       "1                                                NaN         NaN  \n",
       "2  ###[Meta] Sticky Comment\\n\\n[Rule 2](https://w...         NaN  \n",
       "3  And yet...hurricane victims in NC and fire vic...         NaN  \n",
       "4  So they're interfering in international politi...         NaN  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Try specifying a different encoding\n",
    "df = pd.read_csv(\"usaid_reddit_posts_New.csv\", encoding='latin1')  # You can also try 'ISO-8859-1' or 'cp1252'\n",
    "\n",
    "# If it works, the dataframe will load without errors\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\perpe/nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method Downloader.download of <nltk.downloader.Downloader object at 0x0000027745F18F10>>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "\n",
    "# Specify the directory for NLTK data\n",
    "nltk_data_dir = os.path.expanduser(\"~/nltk_data\")\n",
    "nltk.data.path.append(nltk_data_dir)\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords', download_dir=nltk_data_dir)\n",
    "nltk.download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\perpe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\perpe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\perpe/nltk_data'\n    - 'c:\\\\Users\\\\perpe\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\perpe\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\perpe\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\perpe\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\perpe/nltk_data'\n    - 'C:\\\\Users\\\\perpe/nltk_data'\n    - 'C:\\\\Users\\\\perpe/nltk_data'\n    - 'C:\\\\Users\\\\perpe/nltk_data'\n    - 'C:/Users/perpe/nltk_data'\n    - 'C:\\\\Users\\\\perpe/nltk_data'\n    - 'C:\\\\Users\\\\perpe/nltk_data'\n    - 'C:\\\\Users\\\\perpe\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\Users\\\\perpe/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Return an empty string for non-string values\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Apply cleaning to the 'Text' column using .loc\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m df\u001b[38;5;241m.\u001b[39mloc[:, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCleaned_Text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(clean_text)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Save the cleaned dataset\u001b[39;00m\n\u001b[0;32m     18\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcleaned_usaid_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\perpe\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4919\u001b[0m         func,\n\u001b[0;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32mc:\\Users\\perpe\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\perpe\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1509\u001b[0m )\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\perpe\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32mc:\\Users\\perpe\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[59], line 8\u001b[0m, in \u001b[0;36mclean_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      6\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, text)  \u001b[38;5;66;03m# Remove special characters and punctuation\u001b[39;00m\n\u001b[0;32m      7\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, text)\u001b[38;5;241m.\u001b[39mstrip()  \u001b[38;5;66;03m# Remove extra spaces\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m tokens \u001b[38;5;241m=\u001b[39m word_tokenize(text)  \u001b[38;5;66;03m# Tokenize the text\u001b[39;00m\n\u001b[0;32m      9\u001b[0m filtered_words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)]  \u001b[38;5;66;03m# Remove stopwords\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(filtered_words)\n",
      "File \u001b[1;32mc:\\Users\\perpe\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\perpe\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\perpe\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32mc:\\Users\\perpe\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[1;32mc:\\Users\\perpe\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mc:\\Users\\perpe\\anaconda3\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\perpe/nltk_data'\n    - 'c:\\\\Users\\\\perpe\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\perpe\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\perpe\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\perpe\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\perpe/nltk_data'\n    - 'C:\\\\Users\\\\perpe/nltk_data'\n    - 'C:\\\\Users\\\\perpe/nltk_data'\n    - 'C:\\\\Users\\\\perpe/nltk_data'\n    - 'C:/Users/perpe/nltk_data'\n    - 'C:\\\\Users\\\\perpe/nltk_data'\n    - 'C:\\\\Users\\\\perpe/nltk_data'\n    - 'C:\\\\Users\\\\perpe\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\Users\\\\perpe/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Define a cleaning function\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):  # Only process strings\n",
    "        text = text.lower()  # Convert to lowercase\n",
    "        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)  # Remove URLs\n",
    "        text = re.sub(r'\\W', ' ', text)  # Remove special characters and punctuation\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "        tokens = word_tokenize(text)  # Tokenize the text\n",
    "        filtered_words = [word for word in tokens if word not in stopwords.words('english')]  # Remove stopwords\n",
    "        return ' '.join(filtered_words)\n",
    "    else:\n",
    "        return \"\"  # Return an empty string for non-string values\n",
    "\n",
    "# Apply cleaning to the 'Text' column using .loc\n",
    "df.loc[:, 'Cleaned_Text'] = df['Text'].apply(clean_text)\n",
    "\n",
    "# Save the cleaned dataset\n",
    "df.to_csv(\"cleaned_usaid_data.csv\", index=False)\n",
    "print(\"Data cleaned and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cleaned_usaid_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'Cleaned_Text' column contains only strings\n",
    "df['Cleaned_Text'] = df['Cleaned_Text'].astype(str)\n",
    "\n",
    "# Sentiment analysis function\n",
    "def get_sentiment(text):\n",
    "    if not text or text.isspace():  # Check for empty or whitespace-only strings\n",
    "        return \"Neutral\"\n",
    "    analysis = TextBlob(text)\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return \"Positive\"\n",
    "    elif analysis.sentiment.polarity < 0:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "# Apply sentiment analysis to the cleaned text\n",
    "df['Sentiment'] = df['Cleaned_Text'].apply(get_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis completed and data saved.\n"
     ]
    }
   ],
   "source": [
    "# Save the dataset with sentiment results\n",
    "df.to_csv(\"sentiment_usaid_data.csv\", index=False)\n",
    "\n",
    "print(\"Sentiment analysis completed and data saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Post ID', 'region', 'Text', 'title', 'Author', 'subreddit', 'score',\n",
      "       'Upvote Ratio', 'Number of Comments', 'Created At', 'url',\n",
      "       'Created Timestamp', 'comments'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm your USAID information assistant. Ask me anything about USAID.\n",
      "Bot: Here's some information about USAID:\n",
      "- Will US Politics hamper worldwide missionary efforts?: US pollitics are certainly chaotic right now. My suspicion is that the MAGA brand and bashing of non-US peoples and countries will influence foreign perception of US-centered religions, particularly ones from red states. The anti-US rhetoric has escalated from last time around. My bet is that in the next number of years, Mormon recruitment efforts internationally will see declines. Big declines.\n",
      "\n",
      "Is you are Hispanic from central and south America, would you join a church from a state that supports the deportation of your people? If you are African, would you join a church whose people voted for a person that calls your country a \"shithole\" or canceled the support from USAid or thought that anti-apartheid efforts in South Africa were a step backward?\n",
      "\n",
      "From tariff threats, to promoting permanent displacement of Gazaans, to deportation threats, Mormonism will not benefit from the US-centered chaos and messaging. Mormonism will see an uptick internationally as a MAGA brand and not welcoming to non-US people.\n",
      "\n",
      "My bet is foreign baptisms decline internationally over the next 4+ years.\n",
      "\n",
      "Thoughts? \n",
      "- USAID has been completely unaccountable for decades,: From: [https://x.com/rapidresponse47/status/1887151291895267553](https://x.com/rapidresponse47/status/1887151291895267553)\n",
      "\n",
      "USAID has been completely unaccountable for decades, run by bureaucrats with agendas who believed they answered to nobody.  \n",
      "  \n",
      "**Here are a few more of the ridiculous projects on which they spent YOUR money:**  \n",
      "  \n",
      "— $7.9 million to teach Sri Lankan journalists how to avoid “binary-gendered language”  \n",
      "  \n",
      "— $20 million for a new Sesame Street show in Iraq  \n",
      "  \n",
      "— $4.5+ million to “combat disinformation” in Kazakhstan  \n",
      "  \n",
      "— $1.5 million for “art for inclusion of people with disabilities”  \n",
      "  \n",
      "— $2 million for sex changes and “LGBT activism” in Guatemala  \n",
      "  \n",
      "— $6 million to “transform digital spaces to reflect feminist democratic principles”  \n",
      "  \n",
      "— $2.1 million to help the BBC “value the diversity of Libyan society”  \n",
      "  \n",
      "— $10 million worth of USAID-funded meals, which went to an al Qaeda-linked terrorist group  \n",
      "  \n",
      "— $25 million for Deloitte to promote “green transportation” in the country of Georgia  \n",
      "  \n",
      "— $6 million for tourism in Egypt  \n",
      "  \n",
      "— $2.5 million to promote “inclusion” in Vietnam  \n",
      "  \n",
      "— $16.8 million for a SEPARATE “inclusion” group in Vietnam  \n",
      "  \n",
      "— \\~$5 million to EcoHealth Alliance, one of the key NGOs funding bat virus research at the Wuhan lab  \n",
      "  \n",
      "— $20 million for a group related to a key player in the Russiagate impeachment hoax  \n",
      "  \n",
      "— $1.1 million to an Armenian “LGBT group”  \n",
      "  \n",
      "— $1.2 million to help the African Methodist Episcopal Church Service and Development Agency in Washington, D.C., build “a state-of-the-art 440 seat auditorium”  \n",
      "  \n",
      "— $1.3 million to Arab and Jewish photographers  \n",
      "  \n",
      "— $1.5 million to promote “LGBT advocacy” in Jamaica  \n",
      "  \n",
      "— $1.5 million to “rebuild” the Cuban media ecosystem  \n",
      "  \n",
      "— $2 million to promote “LGBT equality through entrepreneurship” in Latin America  \n",
      "  \n",
      "— $500K to solve sectarian violence in Israel (just ten days before the Hamas October 7 attack)  \n",
      "  \n",
      "— $2.3 million for “artisanal and small scale gold mining” in the Amazon  \n",
      "  \n",
      "— $3.9 million for “LGBT causes” in the western Balkans  \n",
      "  \n",
      "— $5.5 million for LGBT activism in Uganda  \n",
      "  \n",
      "— $6 million for advancing LGBT issues in “priority countries around the world”  \n",
      "  \n",
      "— $6.3 million for men who have sex with men in South Africa  \n",
      "  \n",
      "— $8.3 million for “USAID Education: Equity and Inclusion”  \n",
      "  \n",
      "— USAID’s “climate strategy” outlined a $150 billion “whole-of-agency” approach to building an “equitable world with net-zero greenhouse gas emissions.”  \n",
      "  \n",
      "For decades, USAID bureaucrats believed they were accountable to no one — but that era is over.  \n",
      "  \n",
      "President Trump is STOPPING the waste, fraud, and abuse.\n",
      "- I don't think Elon understands how running a country like a company is so bad for everyone.: Elon grew up wealthy and in South Africa which, has a lot of extreme racism against white people. From what I understand, it's pretty bad and difficult to describe how bad. That was his life growing up. Surrounded by anti white racism. \n",
      "\n",
      "He moves to Canada, builds up a few companies and does a pretty good job getting rich off other people's work and motivating people towards a goal. Which is great. All this time however, the people he's using were raised in a country that takes care of its people with a pretty good level of education and healthcare. \n",
      "\n",
      "His move into the US probably sees a similar deal where he secures funding from the Government which, again, takes care of its people generally well and he's able to hire talent but fire whoever he wants. Giving him the perspective of basically dealing with educated, wealthy enough people to get by without a whole lot of health or other issues. \n",
      "\n",
      "So his whole life has been basically trying to run people into the ground to get things done and he's never seen the consequences of burning people out because he's always had the money and ability to just fire and replace as he sees fit. While he's always able to take however much time he wants off and minimal health issues with plenty of resources to deal with it. \n",
      "\n",
      "So now he's in a tall seat of power and he simply does not understand or appreciate the work and effort people have done to hold society together that made it so he had access to those well educated people and the funding he's had in his career. \n",
      "\n",
      "He's had a few rough patches in his life, but they never really impacted him to get him to reflect on his situation or safety nets that kept him from just starving to death on the side of the road. \n",
      "\n",
      "And this is where we see the crossroads between a country and a company. \n",
      "\n",
      "A country has to raise people and take care of them. A company relies on a country's security and social structures to run and sees itself as better than the country. \n",
      "\n",
      "When he's been in USAID, he's been ripping apart and calling programs that help people as criminal and fraud, while ignoring Trumps daughter getting plenty of funding from USAID or the Trumpcoin rugpull, or really anything that's actual corruption. \n",
      "\n",
      "People are getting angry, that tax dollars are used to stabilize and help people. Literally angry at that, and ignoring the fraud that happened from Trumps covid stimulus or other issues. \n",
      "\n",
      "So now he's talking about basically cutting America to the bone and bragging about it because, that's all he knows when it comes to business. You have to be selfish and greedy. That's how businesses work. The opposite of governing where you want families to grow up and be multi generational. \n",
      "\n",
      "I don't know, it's a thought, I needed to get it out there somewhere and see what others think. I want to give Elon some benefit of doubt here. We're watching a bunch of people with what could be considered a retirement income that never have to work for money ever again in their lives, seem to want to do nothing but make people as miserable and poor as possible and it's just, weird.\n",
      "- Please share this on all social! What USAID would do with the $40 million that Leon spent on a super bowl ad: Elon Musk reportedly spent $40 million dollars on #Superbowl ads calling #USAID\n",
      "wasteful. Here are just a few ways we would’ve spent $40M:\n",
      "\n",
      "https://youtu.be/PqUESHfuO_8?si=Iyu0uR8nSQKNzxpl\n",
      "\n",
      "🏈Purchased life-saving HIV treatment from American pharmaceutical companies to keep\n",
      "1,060,000 million people alive for one year\n",
      "🏈Bought enough emergency food-aid products made in Georgia and Rhode Island to treat\n",
      "880,000 severely malnourished children\n",
      "🏈Slowed the flow of migrants to our borders by providing job training and support to\n",
      "50,000 people in Central America\n",
      "🏈Protected 30,000,000 football fields of tropical forest in Africa, preventing China and\n",
      "Russia from exploiting timber and critical minerals that fuel our world\n",
      "🏈Shipped the 29,000 metric tons of American food products worth $40 million on\n",
      "American ships instead of letting it waste away in a Texas warehouse\n",
      "🏈Saved the lives of 25 million children suffering from severe malaria. We think this one\n",
      "has value on its own but maybe Musk would disagree\n",
      "This Superbowl Sunday, tell Congress to #saveforeignaid and #standwithUSAID\n",
      "- USAID Superbowl Ad Resistance Action: Hello everyone! \n",
      "\n",
      "Please see some information below that was sent to me from my network. Felt like everyone here might be intestested. \n",
      "\n",
      "GLOBAL MOMENT OF SOLIDARITY ON SUNDAY, FEBRUARY 9TH @ 5:30PM EST\n",
      "\n",
      "Influencers, friends, family and people who believe in the power of foreign aid.\n",
      "\n",
      "exactly 5:30pm EST. And tell them to spread the word.l media platforms today at\n",
      "\n",
      "INSTAGRAM (share this @friendsofusaid post at 5:30PM EST)\n",
      "https://www.instagram.com/p/DF2wf0tgtKM/\n",
      "\n",
      "FACEBOOK/LINKEDIN (publish the post below at 5:30pm EST)\n",
      "\n",
      "wasteful. Here are just a few ways we would’ve spent $40M:wl ads calling #USAID\n",
      "https://youtu.be/PqUESHfuO_8\n",
      "\n",
      "🏈Purchased life-saving HIV treatment from American pharmaceutical companies to keep 1,060,000 million people alive for one year\n",
      "\n",
      "880,000 severely malnourished childrenducts made in Georgia and Rhode Island to treat\n",
      "\n",
      "50,000 people in Central America our borders by providing job training and support to\n",
      "\n",
      "Russia from exploiting timber and critical minerals that fuel our worldeventing China and\n",
      "\n",
      "American ships instead of letting it waste away in a Texas warehouse million on\n",
      "has value on its own but maybe Musk would disagree from severe malaria. We think this one\n",
      "\n",
      "This Superbowl Sunday, tell Congress to #saveforeignaid and #standwithUSAID\n",
      "TWITTER/X/BLUESKY (publish the posts below at 5:30pm EST)\n",
      "\n",
      "\n",
      "Bot: Sorry, that's beyond my current scope. Let’s talk about something else.\n",
      "Bot: Sorry, that's beyond my current scope. Let’s talk about something else.\n",
      "Bot: Sorry, that's beyond my current scope. Let’s talk about something else.\n",
      "Bot: Sorry, that's beyond my current scope. Let’s talk about something else.\n",
      "Bot: Sorry, that's beyond my current scope. Let’s talk about something else.\n",
      "Bot: Sorry, that's beyond my current scope. Let’s talk about something else.\n",
      "Bot: Sorry, that's beyond my current scope. Let’s talk about something else.\n",
      "Bot: Sorry, that's beyond my current scope. Let’s talk about something else.\n",
      "Bot: Sorry, that's beyond my current scope. Let’s talk about something else.\n",
      "Bot: Sorry, that's beyond my current scope. Let’s talk about something else.\n",
      "Bot: Sorry, that's beyond my current scope. Let’s talk about something else.\n",
      "Bot: Sorry, that's beyond my current scope. Let’s talk about something else.\n",
      "Bot: Sorry, that's beyond my current scope. Let’s talk about something else.\n",
      "Bot: Sorry, that's beyond my current scope. Let’s talk about something else.\n"
     ]
    }
   ],
   "source": [
    "def chatbot():\n",
    "    print(\"Hello! I'm your USAID information assistant. Ask me anything about USAID.\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \").lower()\n",
    "        \n",
    "        if 'usaid' in user_input:\n",
    "            # Query your dataset for relevant information\n",
    "            relevant_data = data[data['Text'].str.contains('usaid', case=False, na=False)]\n",
    "            \n",
    "            if not relevant_data.empty:\n",
    "                print(\"Bot: Here's some information about USAID:\")\n",
    "                for index, row in relevant_data.head().iterrows():\n",
    "                    print(f\"- {row['title']}: {row['Text']}\")\n",
    "            else:\n",
    "                print(\"Bot: Sorry, I couldn't find any information about USAID.\")\n",
    "        else:\n",
    "            print(\"Bot: Sorry, that's beyond my current scope. Let’s talk about something else.\")\n",
    "        \n",
    "        # Exit condition\n",
    "        if user_input in ['exit', 'quit', 'bye']:\n",
    "            print(\"Bot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "# Run the chatbot\n",
    "chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"sentiment_usaid_data.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm your USAID information assistant. Ask me anything about USAID.\n",
      "Bot: Sorry, that's beyond my current scope. Let’s talk about something else.\n",
      "Bot: Here's some information about USAID:\n",
      "- Will US Politics hamper worldwide missionary efforts?: US pollitics are certainly chaotic right now. My suspicion is that the MAGA brand and bashing of non-US peoples and countries will influence foreign perception of US-centered religions, particularly o...\n",
      "- USAID has been completely unaccountable for decades,: From: [https://x.com/rapidresponse47/status/1887151291895267553](https://x.com/rapidresponse47/status/1887151291895267553)\n",
      "\n",
      "USAID has been completely unaccountable for decades, run by bureaucrats with...\n",
      "- I don't think Elon understands how running a country like a company is so bad for everyone.: Elon grew up wealthy and in South Africa which, has a lot of extreme racism against white people. From what I understand, it's pretty bad and difficult to describe how bad. That was his life growing u...\n",
      "- Please share this on all social! What USAID would do with the $40 million that Leon spent on a super bowl ad: Elon Musk reportedly spent $40 million dollars on #Superbowl ads calling #USAID\n",
      "wasteful. Here are just a few ways we would’ve spent $40M:\n",
      "\n",
      "https://youtu.be/PqUESHfuO_8?si=Iyu0uR8nSQKNzxpl\n",
      "\n",
      "🏈Purchased...\n",
      "- USAID Superbowl Ad Resistance Action: Hello everyone! \n",
      "\n",
      "Please see some information below that was sent to me from my network. Felt like everyone here might be intestested. \n",
      "\n",
      "GLOBAL MOMENT OF SOLIDARITY ON SUNDAY, FEBRUARY 9TH @ 5:30PM ES...\n",
      "Bot: Sorry, that's beyond my current scope. Let’s talk about something else.\n",
      "Bot: Sorry, that's beyond my current scope. Let’s talk about something else.\n",
      "Bot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "def chatbot():\n",
    "    print(\"Hello! I'm your USAID information assistant. Ask me anything about USAID.\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \").lower()\n",
    "        \n",
    "        if 'usaid' in user_input:\n",
    "            # Query your dataset for relevant information\n",
    "            relevant_data = df[df['Text'].str.contains('usaid', case=False, na=False)]\n",
    "            \n",
    "            if not relevant_data.empty:\n",
    "                print(\"Bot: Here's some information about USAID:\")\n",
    "                for index, row in relevant_data.head().iterrows():\n",
    "                    # Extract and summarize key points about USAID\n",
    "                    text = row['Text']\n",
    "                    if len(text) > 200:  # Summarize long text\n",
    "                        text = text[:200] + \"...\"\n",
    "                    print(f\"- {row['title']}: {text}\")\n",
    "            else:\n",
    "                print(\"Bot: Sorry, I couldn't find any information about USAID.\")\n",
    "        else:\n",
    "            print(\"Bot: Sorry, that's beyond my current scope. Let’s talk about something else.\")\n",
    "        \n",
    "        # Exit condition\n",
    "        if user_input in ['exit', 'quit', 'bye']:\n",
    "            print(\"Bot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "# Run the chatbot\n",
    "chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Replace with your Deepseek API key\n",
    "DEEPSEEK_API_KEY = \"sk-183372fafc184722ba529f2285c3539b\"\n",
    "DEEPSEEK_SUMMARIZE_URL = \"https://api.deepseek.com/v1/summarize\"  # Replace with the actual API endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm your USAID information assistant. Ask me anything about USAID.\n",
      "Bot: Here's some information about USAID:\n",
      "Error: 404 - {\n",
      "  \"error_msg\": \"Not Found. Please check the configuration.\"\n",
      "}\n",
      "- Will US Politics hamper worldwide missionary efforts?: US pollitics are certainly chaotic right now. My suspicion is that the MAGA brand and bashing of non...\n",
      "Error: 404 - {\n",
      "  \"error_msg\": \"Not Found. Please check the configuration.\"\n",
      "}\n",
      "- USAID has been completely unaccountable for decades,: From: [https://x.com/rapidresponse47/status/1887151291895267553](https://x.com/rapidresponse47/statu...\n",
      "Error: 404 - {\n",
      "  \"error_msg\": \"Not Found. Please check the configuration.\"\n",
      "}\n",
      "- I don't think Elon understands how running a country like a company is so bad for everyone.: Elon grew up wealthy and in South Africa which, has a lot of extreme racism against white people. Fr...\n",
      "Error: 404 - {\n",
      "  \"error_msg\": \"Not Found. Please check the configuration.\"\n",
      "}\n",
      "- Please share this on all social! What USAID would do with the $40 million that Leon spent on a super bowl ad: Elon Musk reportedly spent $40 million dollars on #Superbowl ads calling #USAID\n",
      "wasteful. Here are j...\n",
      "Error: 404 - {\n",
      "  \"error_msg\": \"Not Found. Please check the configuration.\"\n",
      "}\n",
      "- USAID Superbowl Ad Resistance Action: Hello everyone! \n",
      "\n",
      "Please see some information below that was sent to me from my network. Felt like e...\n",
      "Bot: Sorry, that's beyond my current scope. Let’s talk about something else.\n",
      "Bot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset\n",
    "data = pd.read_csv('sentiment_usaid_data.csv')\n",
    "\n",
    "# Function to summarize text using Deepseek API\n",
    "def summarize_with_deepseek(text):\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {DEEPSEEK_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"text\": text,\n",
    "        \"max_length\": 100  # Adjust the summary length as needed\n",
    "    }\n",
    "    response = requests.post(DEEPSEEK_SUMMARIZE_URL, headers=headers, json=payload)\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get(\"summary\", \"No summary available.\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return text[:100] + \"...\"  # Fallback to truncation if API fails\n",
    "\n",
    "# Chatbot function\n",
    "def chatbot():\n",
    "    print(\"Hello! I'm your USAID information assistant. Ask me anything about USAID.\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \").lower()\n",
    "        \n",
    "        if 'usaid' in user_input:\n",
    "            # Query your dataset for relevant information\n",
    "            relevant_data = data[data['Text'].str.contains('usaid', case=False, na=False)]\n",
    "            \n",
    "            if not relevant_data.empty:\n",
    "                print(\"Bot: Here's some information about USAID:\")\n",
    "                for index, row in relevant_data.head().iterrows():\n",
    "                    # Summarize the text using Deepseek API\n",
    "                    summary = summarize_with_deepseek(row['Text'])\n",
    "                    print(f\"- {row['title']}: {summary}\")\n",
    "            else:\n",
    "                print(\"Bot: Sorry, I couldn't find any information about USAID.\")\n",
    "        else:\n",
    "            print(\"Bot: Sorry, that's beyond my current scope. Let’s talk about something else.\")\n",
    "        \n",
    "        # Exit condition\n",
    "        if user_input in ['exit', 'quit', 'bye']:\n",
    "            print(\"Bot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "# Run the chatbot\n",
    "chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIStatusError",
     "evalue": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAPIStatusError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[0;32m      5\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msk-183372fafc184722ba529f2285c3539b\u001b[39m\u001b[38;5;124m\"\u001b[39m, base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.deepseek.com\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m      8\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeepseek-chat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     10\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m     11\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m     12\u001b[0m     ],\n\u001b[0;32m     13\u001b[0m     stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[1;32mc:\\Users\\perpe\\anaconda3\\Lib\\site-packages\\openai\\_utils\\_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\perpe\\anaconda3\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:829\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    826\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    827\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    828\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    830\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    831\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    832\u001b[0m             {\n\u001b[0;32m    833\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    834\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    835\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[0;32m    836\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    837\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    838\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    839\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    840\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    841\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m    842\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    843\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    844\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[0;32m    845\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    846\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m    847\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[0;32m    848\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    849\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    850\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    851\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m    852\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    853\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m    854\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    855\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m    856\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    857\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    858\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    859\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    860\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    861\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    862\u001b[0m             },\n\u001b[0;32m    863\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    864\u001b[0m         ),\n\u001b[0;32m    865\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    866\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    867\u001b[0m         ),\n\u001b[0;32m    868\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    869\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    870\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    871\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\perpe\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1280\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1267\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1268\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1275\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1277\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1278\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1279\u001b[0m     )\n\u001b[1;32m-> 1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32mc:\\Users\\perpe\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:957\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    955\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 957\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    958\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    959\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    960\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    961\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    962\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m    963\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\perpe\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1061\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1058\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1060\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1061\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1064\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1065\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1069\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1070\u001b[0m )\n",
      "\u001b[1;31mAPIStatusError\u001b[0m: Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
     ]
    }
   ],
   "source": [
    "# Please install OpenAI SDK first: `pip3 install openai`\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"sk-183372fafc184722ba529f2285c3539b\", base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello\"},\n",
    "    ],\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
